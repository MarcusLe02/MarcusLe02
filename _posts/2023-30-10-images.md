---
layout: post
title: "Reinforcement Learning explained: theory"
date: 2023-10-30 21:01:00
description: this is what included images could look like
tags: formatting images
categories: sample-posts
thumbnail: assets/img/9.jpg
---

Machine Learning has become a buzzword, not just for data enthusiasts but also for the general public, particularly with the advent of technologies like ChatGPT. Typically, when we talk about Machine Learning, two main types come to mind: Supervised Learning and Unsupervised Learning, known for their roles in recognizing patterns and making predictions. However, there’s a third, less-talked-about but equally fascinating sibling that goes beyond data prediction and generation: Reinforcement Learning (RL).


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/9.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<h3>Understanding Reinforcement Learning</h3>

At its core, Reinforcement Learning is about an agent (you can think of an agent like a robot) learning to make decisions by interacting with its environment. The goal is to perform actions that yield the highest cumulative rewards over time. What sets RL apart from other Machine Learning methods is that it doesn’t rely on pre-labeled data or historical examples. Instead, the agent learns through a process of trial and error in many iterations, making decisions and adjusting based on the outcomes of previous iterations.

<h3>Characteristics of RL</h3>

<ul>
  <li><strong>Learning From Scratch:</strong> Unlike Supervised Learning, RL does not provide the agent datasets with inputs-output. Instead, the agent begins with minimal knowledge of its environment and learns by exploring.</li>
  
  <li><strong>No Direct Supervisor:</strong> There’s no external ‘teacher’ giving correct answers for the agent. It accumulates knowledge by understanding the consequences of its actions.</li>
  
  <li><strong>Delayed Feedback:</strong> Unlike typical Machine Learning, rewards in RL are often not immediate. The impact of actions may unfold over time, requiring the agent to think long-term.</li>
  
  <li><strong>Interactive Learning Cycle:</strong> The agent’s actions shape its future experiences, creating a dynamic learning environment.</li>
</ul>

<h3>Markov Decision Process – The Framework of RL</h3>

To understand RL, it’s essential to grasp the concept of a Markov Decision Process (MDP). This framework comprises five elements:

<ul>
  <li><strong>Agent:</strong> The learner that makes decisions.</li>

  <li><strong>Environment:</strong> The world in which the agent operates, presenting different states.</li>

  <li><strong>State:</strong> The current situation or context the agent finds itself in.</li>

  <li><strong>Action:</strong> What the agent can do at each state, to transition from one state to another.</li>

  <li><strong>Reward:</strong> Feedback from the environment indicating the success of an action.</li>
</ul>

If this is your first time hearing about a Markov Decision Process (MDP), it might seem quite complex. Let’s simplify it with an example. Consider a chess game. In this context, the arrangement of pieces on the current board represents the ‘state.’ The moves you can make are the ‘actions,’ and the benefits you gain, like capturing an opponent’s piece, having a favorable board state, or ultimately winning the game, are the ‘rewards.’ The player, or in RL terms, the ‘agent,’ makes decisions (moves) aimed at maximizing these rewards.

Below is a sample Markov chain for a day in a student’s life. Can you tell what are the agent, environment, states, actions, and rewards?

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/8.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<h3>Markov Decision Process – The Framework of RL</h3>

Building on our chess game example, let’s delve into the iterative interaction process in a Markov Decision Process (MDP), using a chess engine like Stockfish to understand how an MDP works.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/11.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Stockfish 16, the most powerful chess engine in the world
</div>

<ol>
  <li><strong>Initial State and Action:</strong> Each iteration in the game begins with the engine observing the current state of the chessboard. It evaluates the positions of all pieces (both its own and the opponent’s) and decides on its next move. This move is the chosen ‘action’ in MDP terms.</li>

  <li><strong>Environment Response and Transition to New State:</strong> Once the engine executes its move, the chessboard (the environment) changes to a new state. This is the result of the action taken by the engine and the subsequent move by the opponent.</li>

  <li><strong>Reward Assessment and Strategy Adjustment:</strong> The engine receives feedback after each move. For example, if a move results in capturing an opponent’s piece, the engine might interpret this as a positive reward. This feedback helps the engine assess the effectiveness of its strategy and adjust future actions.</li>

  <li><strong>Continuous Learning Through Game Progression:</strong> As the game progresses, the engine continuously refines its strategy. It learns from the outcomes of previous actions and adapts to the changing dynamics of the game. Each move is a step in the learning process, aiming to improve the chances of success in future states.</li>

  <li><strong>Concluding an Episode:</strong> In the context of chess, a training episode concludes at the end of a single game. The engine then analyzes the overall outcome, considering the cumulative rewards and the effectiveness of its strategy throughout the game.</li>

  <li><strong>Beginning a New Episode:</strong> Upon starting a new game, the engine resets to a new initial state, ready to apply its refined strategy and learnings from previous games.</li>
</ol>

This iterative process illustrates how the chess engine, acting as an RL agent within an MDP framework, continually learns and adapts its strategies with each move, seeking to optimize its gameplay over time. In reality, the training phase happens in hundreds or thousands of episodes – each game or ‘episode’ contributing to the engine’s growing experience. Over these episodes, the engine accumulates a vast array of game situations and outcomes, allowing it to refine its decision-making algorithms. This extensive training enables the engine to recognize patterns, anticipate opponent strategies, and evaluate the long-term consequences of moves.

<h3>Return, Policy, and Value</h3>

As we delve deeper into Reinforcement Learning, there are three additional key terms that are essential for anyone learning RL. These terms are not just theoretical concepts but are actively utilized in the practical coding of RL algorithms.

<ul>
  <li><strong>Return:</strong> In Reinforcement Learning, ‘return’ refers to the total amount of reward an agent accumulates over time. For our chess engine, it’s about consistently making moves that lead to success across many games. The return is a measure of the long-term effectiveness of the engine’s strategies.</li>

  <li><strong>Policy:</strong> The ‘policy’ is the strategy or set of rules that the agent follows while making decisions. In the chess engine’s case, this involves complex algorithms that determine the best move in each game state. A good policy balances immediate move benefits with potential future advantages.</li>

  <li><strong>Value:</strong> ‘Value’ in RL signifies the expected long-term return from a particular state under the current policy. It helps the agent assess which positions on the chessboard are most advantageous. High-value states are those from which the engine has a better chance of winning in the long run.</li>
</ul>

<strong>Note</strong>: In this blog, we have only touched upon these terms briefly. It’s important to understand that there’s much more complex mathematics involved in fully grasping these concepts. For those interested in diving deeper in Return, Policy, and Value in Reinforcement Learning, this comprehensive guide provides an in-depth exploration.

<h3>Moving forward</h3>

We have now explored the foundational aspects of Reinforcement Learning, delving into key concepts and how they play a critical role in the strategic decision-making of an RL agent. These concepts, while presented in a simplified form here, form the backbone of any RL algorithm.

As we move forward, our journey into Reinforcement Learning takes a practical turn. In the next post, we will shift from theory to application, focusing a coding implementation of RL. This upcoming post will provide hands-on insights into how these theoretical concepts are translated into functioning RL algorithms. We will walk through specific coding examples, and demonstrate step-by-step on how to build our first deep-learning RL model.